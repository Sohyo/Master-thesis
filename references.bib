@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={233--242},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 19)},
  pages={267--284},
  year={2019}
}
@inproceedings{cancedda2012private,
  title={Private access to phrase tables for statistical machine translation},
  author={Cancedda, Nicola},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={23--27},
  year={2012}
}

@inproceedings{galle2015reconstructing,
  title={Reconstructing Textual Documents from n-grams},
  author={Gall{\'e}, Matthias and Tealdi, Mat{\'\i}as},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={329--338},
  year={2015}
}

@article{michel2011quantitative,
  title={Quantitative analysis of culture using millions of digitized books},
  author={Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K and Pickett, Joseph P and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and others},
  journal={science},
  volume={331},
  number={6014},
  pages={176--182},
  year={2011},
  publisher={American Association for the Advancement of Science}
}

@article{young2018recent,
  title={Recent trends in deep learning based natural language processing},
  author={Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  journal={ieee Computational intelligenCe magazine},
  volume={13},
  number={3},
  pages={55--75},
  year={2018},
  publisher={IEEE}
}

@article{zoph2016transfer,
  title={Transfer learning for low-resource neural machine translation},
  author={Zoph, Barret and Yuret, Deniz and May, Jonathan and Knight, Kevin},
  journal={arXiv preprint arXiv:1604.02201},
  year={2016}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{tiedemann-2012-parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
    abstract = "This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.",
}

@article{steinberger2006jrc,
  title={The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages},
  author={Steinberger, Ralf and Pouliquen, Bruno and Widiger, Anna and Ignat, Camelia and Erjavec, Tomaz and Tufis, Dan and Varga, D{\'a}niel},
  journal={arXiv preprint cs/0609058},
  year={2006}
}

@inproceedings{koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Hoang, Hieu  and
      Birch, Alexandra  and
      Callison-Burch, Chris  and
      Federico, Marcello  and
      Bertoldi, Nicola  and
      Cowan, Brooke  and
      Shen, Wade  and
      Moran, Christine  and
      Zens, Richard  and
      Dyer, Chris  and
      Bojar, Ond{\v{r}}ej  and
      Constantin, Alexandra  and
      Herbst, Evan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-2045",
    pages = "177--180",
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{aulamo-etal-2020-opustools,
    title = "{O}pus{T}ools and Parallel Corpus Diagnostics",
    author = {Aulamo, Mikko  and
      Sulubacak, Umut  and
      Virpioja, Sami  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of The 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.467",
    pages = "3782--3789",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{koehn-etal-2003-statistical,
    title = "Statistical Phrase-Based Translation",
    author = "Koehn, Philipp  and
      Och, Franz J.  and
      Marcu, Daniel",
    booktitle = "Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2003",
    url = "https://www.aclweb.org/anthology/N03-1017",
    pages = "127--133",
}

@inproceedings{koehn-etal-2003-statistical,
    title = "Statistical Phrase-Based Translation",
    author = "Koehn, Philipp  and
      Och, Franz J.  and
      Marcu, Daniel",
    booktitle = "Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2003",
    url = "https://www.aclweb.org/anthology/N03-1017",
    pages = "127--133",
}

@inproceedings{koehn2003statistical,
    title = "Statistical Phrase-Based Translation",
    author = "Koehn, Philipp  and
      Och, Franz J.  and
      Marcu, Daniel",
    booktitle = "Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2003",
    url = "https://www.aclweb.org/anthology/N03-1017",
    pages = "127--133",
}

@inproceedings{luong2015stanford,
  title={Stanford neural machine translation systems for spoken language domains},
  author={Luong, Minh-Thang and Manning, Christopher D and others},
  booktitle={Proceedings of the international workshop on spoken language translation},
  pages={76--79},
  year={2015}
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@inproceedings{sennrich2016controlling,
  title={Controlling politeness in neural machine translation via side constraints},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={35--40},
  year={2016}
}

@inproceedings{tiedemann2012parallel,
  title={Parallel Data, Tools and Interfaces in OPUS.},
  author={Tiedemann, J{\"o}rg},
  booktitle={Lrec},
  volume={2012},
  pages={2214--2218},
  year={2012}
}

@inproceedings{ng-etal-2019-,
    title = "{F}acebook {FAIR}{'}s {WMT}19 News Translation Task Submission",
    author = "Ng, Nathan  and
      Yee, Kyra  and
      Baevski, Alexei  and
      Ott, Myle  and
      Auli, Michael  and
      Edunov, Sergey",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5333",
    doi = "10.18653/v1/W19-5333",
    pages = "314--319",
    abstract = "This paper describes Facebook FAIR{'}s submission to the WMT19 shared news translation task. We participate in four language directions, English {\textless}-{\textgreater} German and English {\textless}-{\textgreater} Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system{'}s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction Englishâ†’Russian.",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}


@inproceedings{thompson-etal-2019-hablex,
    title = "{HABL}ex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation",
    author = "Thompson, Brian  and
      Knowles, Rebecca  and
      Zhang, Xuan  and
      Khayrallah, Huda  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1142",
    doi = "10.18653/v1/D19-1142",
    pages = "1382--1387",
    abstract = "Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human generated alignments of words and phrases in machine translation test sets in three language pairs (Russian-English, Chinese-English, and Korean-English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines - constrained decoding and continued training - and an improvement to continued training to address overfitting.",
}

@inproceedings{barrault2019findings,
  title={Findings of the 2019 conference on machine translation (wmt19)},
  author={Barrault, Lo{\"\i}c and Bojar, Ond{\v{r}}ej and Costa-Jussa, Marta R and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Koehn, Philipp and Malmasi, Shervin and others},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  pages={1--61},
  year={2019}
}

@article{hutchins2007machine,
  title={Machine translation: A concise history},
  author={Hutchins, John},
  journal={Computer aided translation: Theory and practice},
  volume={13},
  number={29-70},
  pages={11},
  year={2007},
  publisher={Chinese University of Hong Kong}
}

@inproceedings{koehn-knowles-2017-six,
    title = "Six Challenges for Neural Machine Translation",
    author = "Koehn, Philipp  and
      Knowles, Rebecca",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-3204",
    doi = "10.18653/v1/W17-3204",
    pages = "28--39",
    abstract = "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
}

@inproceedings{cancedda-2012-private,
    title = "Private Access to Phrase Tables for Statistical Machine Translation",
    author = "Cancedda, Nicola",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-2005",
    pages = "23--27",
}

@inproceedings{sutskever2014sequence,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{ott-etal-2019-fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}

@inproceedings{ng-etal-2019-facebook,
    title = "{F}acebook {FAIR}{'}s {WMT}19 News Translation Task Submission",
    author = "Ng, Nathan  and
      Yee, Kyra  and
      Baevski, Alexei  and
      Ott, Myle  and
      Auli, Michael  and
      Edunov, Sergey",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5333",
    doi = "10.18653/v1/W19-5333",
    pages = "314--319",
    abstract = "This paper describes Facebook FAIR{'}s submission to the WMT19 shared news translation task. We participate in four language directions, English {\textless}-{\textgreater} German and English {\textless}-{\textgreater} Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system{'}s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction Englishâ†’Russian.",
}

@inproceedings{barrault-etal-2019-findings,
    title = "Findings of the 2019 Conference on Machine Translation ({WMT}19)",
    author = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Koehn, Philipp  and
      Malmasi, Shervin  and
      Monz, Christof  and
      M{\"u}ller, Mathias  and
      Pal, Santanu  and
      Post, Matt  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5301",
    doi = "10.18653/v1/W19-5301",
    pages = "1--61",
    abstract = "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.",
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@inproceedings{dyer-etal-2013-simple,
    title = "A Simple, Fast, and Effective Reparameterization of {IBM} Model 2",
    author = "Dyer, Chris  and
      Chahuneau, Victor  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1073",
    pages = "644--648",
}

@inproceedings{koehn-etal-2003-statistical,
    title = "Statistical Phrase-Based Translation",
    author = "Koehn, Philipp  and
      Och, Franz J.  and
      Marcu, Daniel",
    booktitle = "Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2003",
    url = "https://www.aclweb.org/anthology/N03-1017",
    pages = "127--133",
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@book{koehn_2009, place={Cambridge}, title={Statistical Machine Translation}, DOI={10.1017/CBO9780511815829}, publisher={Cambridge University Press}, author={Koehn, Philipp}, year={2009}}

@article{irvine-etal-2013-measuring,
    title = "Measuring Machine Translation Errors in New Domains",
    author = "Irvine, Ann  and
      Morgan, John  and
      Carpuat, Marine  and
      Daum{\'e} III, Hal  and
      Munteanu, Dragos",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "1",
    year = "2013",
    url = "https://www.aclweb.org/anthology/Q13-1035",
    doi = "10.1162/tacl_a_00239",
    pages = "429--440",
    abstract = "We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation.",
}

@inproceedings{chu-wang-2018-survey,
    title = "A Survey of Domain Adaptation for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Wang, Rui",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1111",
    pages = "1304--1319",
    abstract = "Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",
}
@inproceedings{miceli-barone-etal-2017-regularization,
    title = "Regularization techniques for fine-tuning in neural machine translation",
    author = "Miceli Barone, Antonio Valerio  and
      Haddow, Barry  and
      Germann, Ulrich  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1156",
    doi = "10.18653/v1/D17-1156",
    pages = "1489--1494",
    abstract = "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for Englishâ†’German and Englishâ†’Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.",
}

@inproceedings{sato-etal-2020-vocabulary,
    title = "Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation",
    author = "Sato, Shoetsu  and
      Sakuma, Jin  and
      Yoshinaga, Naoki  and
      Toyoda, Masashi  and
      Kitsuregawa, Masaru",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.381",
    doi = "10.18653/v1/2020.findings-emnlp.381",
    pages = "4269--4279",
    abstract = "Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between distant domains (e.g., movie subtitles and research papers), however, cannot be performed effectively due to mismatches in vocabulary; it will encounter many domain-specific words (e.g., {``}angstrom{''}) and words whose meanings shift across domains (e.g., {``}conductor{''}). In this study, aiming to solve these vocabulary mismatches in domain adaptation for neural machine translation (NMT), we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pretrained NMT model to the target domain. Prior to fine-tuning, our method replaces the embedding layers of the NMT model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space. Experimental results indicate that our method improves the performance of conventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En translation, respectively.",
}


@inproceedings{kalchbrenner-blunsom-2013-recurrent,
    title = "Recurrent Continuous Translation Models",
    author = "Kalchbrenner, Nal  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1176",
    pages = "1700--1709",
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{ostling2017neural,
  title={Neural machine translation for low-resource languages},
  author={{\"O}stling, Robert and Tiedemann, J{\"o}rg},
  journal={arXiv preprint arXiv:1708.05729},
  year={2017}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@INPROCEEDINGS{kneser-ney,
  author={R. {Kneser} and H. {Ney}},
  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing}, 
  title={Improved backing-off for M-gram language modeling}, 
  year={1995},
  volume={1},
  number={},
  pages={181-184 vol.1},
  doi={10.1109/ICASSP.1995.479394}}
  
  @inproceedings{brants-etal-2007-large,
    title = "Large Language Models in Machine Translation",
    author = "Brants, Thorsten  and
      Popat, Ashok C.  and
      Xu, Peng  and
      Och, Franz J.  and
      Dean, Jeffrey",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D07-1090",
    pages = "858--867",
}

@book{eisenstein2019introduction,
  title={Introduction to natural language processing},
  author={Eisenstein, Jacob},
  year={2019},
  publisher={MIT press}
}

@inproceedings{zhang-etal-2019-bridging,
    title = "Bridging the Gap between Training and Inference for Neural Machine Translation",
    author = "Zhang, Wen  and
      Feng, Yang  and
      Meng, Fandong  and
      You, Di  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1426",
    doi = "10.18653/v1/P19-1426",
    pages = "4334--4343",
    abstract = "Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese-{\textgreater}English and WMT{'}14 English-{\textgreater}German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.",
}

@online{Encyclopedia,
  %author = {For Encyclopedia of American Law},
  title = {Stay. (n.d.) West's Encyclopedia of American Law, edition 2.},
  year = 2008,
  url = {https://legal-dictionary.thefreedictionary.com/Stay},
  urldate = {Retrieved June 10 2021 from}
}

@article{daems2017translation,
  title={Translation methods and experience: A comparative analysis of human translation and post-editing with students and professional translators},
  author={Daems, Joke and Vandepitte, Sonia and Hartsuiker, Robert and Macken, Lieve},
  journal={Meta: Journal des traducteurs/Meta: Translators’ Journal},
  volume={62},
  number={2},
  pages={245--270},
  year={2017},
  publisher={Les Presses de l’Universit{\'e} de Montr{\'e}al}
}

@article{jia2019does,
  title={How does the post-editing of neural machine translation compare with from-scratch translation? A product and process study},
  author={Jia, Yanfang and Carl, Michael and Wang, Xiangling},
  journal={The Journal of Specialised Translation},
  volume={31},
  pages={60--86},
  year={2019}
}

@inproceedings{laubli-etal-2019-post,
    title = "Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain",
    author = {L{\"a}ubli, Samuel  and
      Amrhein, Chantal  and
      D{\"u}ggelin, Patrick  and
      Gonzalez, Beatriz  and
      Zwahlen, Alena  and
      Volk, Martin},
    booktitle = "Proceedings of Machine Translation Summit XVII Volume 1: Research Track",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    url = "https://www.aclweb.org/anthology/W19-6626",
    pages = "267--272",
}

@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{gilbertcan,
author = {Gilbert, Devin},
year = {2020},
month = {04},
pages = {},
title = {Working Paper: What can NMT learn with very little training? Using a small data sample to train a commercially available, customizable neural machine translation system in order to translate literature}
}

@article{park2020decoding,
  title={Decoding strategies for improving low-resource machine translation},
  author={Park, Chanjun and Yang, Yeongwook and Park, Kinam and Lim, Heuiseok},
  journal={Electronics},
  volume={9},
  number={10},
  pages={1562},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{freitag2016fast,
  title={Fast domain adaptation for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser},
  journal={arXiv preprint arXiv:1612.06897},
  year={2016}
}

@inproceedings{chu-etal-2017-empirical,
    title = "An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Dabre, Raj  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-2061",
    doi = "10.18653/v1/P17-2061",
    pages = "385--391",
    abstract = "In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.",
}

@inproceedings{miceli-barone-etal-2017-regularization,
    title = "Regularization techniques for fine-tuning in neural machine translation",
    author = "Miceli Barone, Antonio Valerio  and
      Haddow, Barry  and
      Germann, Ulrich  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1156",
    doi = "10.18653/v1/D17-1156",
    pages = "1489--1494",
    abstract = "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for Englishâ†’German and Englishâ†’Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.",
}

@article{geman1992neural,
  title={Neural networks and the bias/variance dilemma},
  author={Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal={Neural computation},
  volume={4},
  number={1},
  pages={1--58},
  year={1992},
  publisher={MIT Press}
}

@book{goldreich2009foundations,
  title={Foundations of cryptography: volume 2, basic applications},
  author={Goldreich, Oded},
  year={2009},
  publisher={Cambridge university press}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{chu-etal-2017-empirical,
    title = "An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Dabre, Raj  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-2061",
    doi = "10.18653/v1/P17-2061",
    pages = "385--391",
    abstract = "In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.",
}

@inproceedings{och-etal-1999-improved,
    title = "Improved Alignment Models for Statistical Machine Translation",
    author = "Och, Franz Josef  and
      Tillmann, Christoph  and
      Ney, Hermann",
    booktitle = "1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    year = "1999",
    url = "https://www.aclweb.org/anthology/W99-0604",
}

@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}
@inproceedings{thompson-etal-2019-overcoming,
    title = "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation",
    author = "Thompson, Brian  and
      Gwinnup, Jeremy  and
      Khayrallah, Huda  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1209",
    doi = "10.18653/v1/N19-1209",
    pages = "2062--2068",
    abstract = "Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC){---}a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",
}

@article{hutchins2007machine,
  title={Machine translation: A concise history},
  author={Hutchins, John},
  journal={Computer aided translation: Theory and practice},
  volume={13},
  number={29-70},
  pages={11},
  year={2007},
  publisher={Chinese University of Hong Kong}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}

@article{meystre2010automatic,
  title={Automatic de-identification of textual documents in the electronic health record: a review of recent research},
  author={Meystre, Stephane M and Friedlin, F Jeffrey and South, Brett R and Shen, Shuying and Samore, Matthew H},
  journal={BMC medical research methodology},
  volume={10},
  number={1},
  pages={1--16},
  year={2010},
  publisher={BioMed Central}
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@inproceedings{kim-etal-2021-using,
    title = "Using Confidential Data for Domain Adaptation of Neural Machine Translation",
    author = "Kim, Sohyung  and
      Bisazza, Arianna  and
      Turkmen, Fatih",
    booktitle = "Proceedings of the Third Workshop on Privacy in Natural Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.privatenlp-1.6",
    doi = "10.18653/v1/2021.privatenlp-1.6",
    pages = "46--52",
    abstract = "We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a random sample to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.",
}

@inproceedings{laubli-etal-2018-machine,
    title = "Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation",
    author = {L{\"a}ubli, Samuel  and
      Sennrich, Rico  and
      Volk, Martin},
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1512",
    doi = "10.18653/v1/D18-1512",
    pages = "4791--4796",
    abstract = "Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese{--}English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",
}

@article{toral2018post,
  title={Post-editing effort of a novel with statistical and neural machine translation},
  author={Toral, Antonio and Wieling, Martijn and Way, Andy},
  journal={Frontiers in Digital Humanities},
  volume={5},
  pages={9},
  year={2018},
  publisher={Frontiers}
}

@inproceedings{kobus-etal-2017-domain,
    title = "Domain Control for Neural Machine Translation",
    author = "Kobus, Catherine  and
      Crego, Josep  and
      Senellart, Jean",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_049",
    doi = "10.26615/978-954-452-049-6_049",
    pages = "372--378",
    abstract = "Machine translation systems are very sensitive to the domains they were trained on. Several domain adaptation techniques have already been deeply studied. We propose a new technique for neural machine translation (NMT) that we call domain control which is performed at runtime using a unique neural network covering multiple domains. The presented approach shows quality improvements when compared to dedicated domains translating on any of the covered domains and even on out-of-domain data. In addition, model parameters do not need to be re-estimated for each domain, making this effective to real use cases. Evaluation is carried out on English-to-French translation for two different testing scenarios. We first consider the case where an end-user performs translations on a known domain. Secondly, we consider the scenario where the domain is not known and predicted at the sentence level before translating. Results show consistent accuracy improvements for both conditions.",
}

@article{kuczmarski2018gender,
  title={Gender-aware natural language translation},
  author={Kuczmarski, James and Johnson, Melvin},
  year={2018}
}

@article{johnson-etal-2017-googles,
    title = "{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1024",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
}

@inproceedings{thompson-etal-2019-hablex,
    title = "{HABL}ex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation",
    author = "Thompson, Brian  and
      Knowles, Rebecca  and
      Zhang, Xuan  and
      Khayrallah, Huda  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1142",
    doi = "10.18653/v1/D19-1142",
    pages = "1382--1387",
    abstract = "Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human generated alignments of words and phrases in machine translation test sets in three language pairs (Russian-English, Chinese-English, and Korean-English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines - constrained decoding and continued training - and an improvement to continued training to address overfitting.",
}

@InProceedings{pmlr-v80-ott18a,
  title = 	 {Analyzing Uncertainty in Neural Machine Translation},
  author =       {Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marc'Aurelio},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3956--3965},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ott18a/ott18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/ott18a.html},
  abstract = 	 {Machine translation is a popular test bed for research in neural sequence-to-sequence models but despite much recent research, there is still a lack of understanding of these models. Practitioners report performance degradation with large beams, the under-estimation of rare words and a lack of diversity in the final translations. Our study relates some of these issues to the inherent uncertainty of the task, due to the existence of multiple valid translations for a single source sentence, and to the extrinsic uncertainty caused by noisy training data. We propose tools and metrics to assess how uncertainty in the data is captured by the model distribution and how it affects search strategies that generate translations. Our results show that search works remarkably well but that the models tend to spread too much probability mass over the hypothesis space. Next, we propose tools to assess model calibration and show how to easily fix some shortcomings of current models. We release both code and multiple human reference translations for two popular benchmarks.}
}


@online{iconictranslation,
  author = {Iconic-Translation-Machines-Ltd},
  title = {Machine Translation Quality},
  year = "2019",
  url = {https://iconictranslation.com/what-we-do/neural-machine-translation/}
}

@inproceedings{lagarda-etal-2009-statistical,
    title = "Statistical Post-Editing of a Rule-Based Machine Translation System",
    author = "Lagarda, Antonio-L.  and
      Alabau, Vicent  and
      Casacuberta, Francisco  and
      Silva, Roberto  and
      D{\'\i}az-de-Lia{\~n}o, Enrique",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N09-2055",
    pages = "217--220",
}

@article{brown-etal-1993-mathematics,
    title = "The Mathematics of Statistical Machine Translation: Parameter Estimation",
    author = "Brown, Peter F.  and
      Della Pietra, Stephen A.  and
      Della Pietra, Vincent J.  and
      Mercer, Robert L.",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    url = "https://aclanthology.org/J93-2003",
    pages = "263--311",
}

@inproceedings{berger-etal-1994-candide,
    title = "The {C}andide System for Machine Translation",
    author = "Berger, Adam L.  and
      Brown, Peter F.  and
      Della Pietra, Stephen A.  and
      Della Pietra, Vincent J.  and
      Gillett, John R.  and
      Lafferty, John D.  and
      Mercer, Robert L.  and
      Printz, Harry  and
      Ures, Lubos",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",
    year = "1994",
    url = "https://aclanthology.org/H94-1028",
}


@inproceedings{koehn-etal-2003-statistical,
    title = "Statistical Phrase-Based Translation",
    author = "Koehn, Philipp  and
      Och, Franz J.  and
      Marcu, Daniel",
    booktitle = "Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2003",
    url = "https://aclanthology.org/N03-1017",
    pages = "127--133",
}
@inproceedings{bentivogli-etal-2016-neural,
    title = "Neural versus Phrase-Based Machine Translation Quality: a Case Study",
    author = "Bentivogli, Luisa  and
      Bisazza, Arianna  and
      Cettolo, Mauro  and
      Federico, Marcello",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1025",
    doi = "10.18653/v1/D16-1025",
    pages = "257--267",
}

@article{feng2020securenlp,
  title={SecureNLP: A system for multi-party privacy-preserving natural language processing},
  author={Feng, Qi and He, Debiao and Liu, Zhe and Wang, Huaqun and Choo, Kim-Kwang Raymond},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3709--3721},
  year={2020},
  publisher={IEEE}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@article{riazi2019deep,
  title={Deep learning on private data},
  author={Riazi, M Sadegh and Rouani, Bita Darvish and Koushanfar, Farinaz},
  journal={IEEE Security \& Privacy},
  volume={17},
  number={6},
  pages={54--63},
  year={2019},
  publisher={IEEE}
}
@inproceedings{huang-etal-2020-texthide,
    title = "{T}ext{H}ide: Tackling Data Privacy in Language Understanding Tasks",
    author = "Huang, Yangsibo  and
      Song, Zhao  and
      Chen, Danqi  and
      Li, Kai  and
      Arora, Sanjeev",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.123",
    doi = "10.18653/v1/2020.findings-emnlp.123",
    pages = "1368--1382",
    abstract = "An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9{\%}. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.",
}

@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@article{al2020privft,
  title={Privft: Private and fast text classification with homomorphic encryption},
  author={Al Badawi, Ahmad and Hoang, Louie and Mun, Chan Fook and Laine, Kim and Aung, Khin Mi Mi},
  journal={IEEE Access},
  volume={8},
  pages={226544--226556},
  year={2020},
  publisher={IEEE}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}
@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@article{hassan2018achieving,
  title={Achieving human parity on automatic chinese to english news translation},
  author={Hassan, Hany and Aue, Anthony and Chen, Chang and Chowdhary, Vishal and Clark, Jonathan and Federmann, Christian and Huang, Xuedong and Junczys-Dowmunt, Marcin and Lewis, William and Li, Mu and others},
  journal={arXiv preprint arXiv:1803.05567},
  year={2018}
}
