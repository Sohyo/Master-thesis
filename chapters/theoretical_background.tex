%There are some concepts that the reader should be familiarised with in order to fully understand the work described in the following chapters. These concepts are briefly explained in this chapter in order to contextualize and help the reader understand the rest of this writing.

\section{Neural Machine Translation}

Neural machine translation(NMT) has achieved

\section{Encoder-Decoder Architectures}

Limitations of Encoder-Decoder Networks

\section{Attention based encoder decoder Framework}
\subsection{Transformers}
The Transformer is a deep learning model proposed in the paper Attention is All You Need 
\section{Domain Adaptation for Neural Machine Translation}
\section{Regularization Techniques}
\parencite{arpit2017closer} suggested several regularization techniques to prevent overfitting. \\
\subsection{Early stopping}
\subsection{Dropout}
\subsection{Weight decay}
